{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Survival Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is intended to predict survival of Titanic passengers using Python and its libraries. Many models will be covered with fine-tuned hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from lib.data_utils import load_Titanic, create_submission\n",
    "# scikit_learn\n",
    "from sklearn.preprocessing import scale, LabelEncoder\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "# Keras\n",
    "from keras.layers import Input, Dense, Flatten, Activation\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras import backend as ktf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['0.827377' '1' '-0.59759' '0.432793' '-0.47367' '-0.50244' '2']\n",
      " ['-1.56610' '0' '0.632604' '0.432793' '-0.47367' '0.786845' '0']\n",
      " ['0.827377' '0' '-0.29004' '-0.47454' '-0.47367' '-0.48885' '2']\n",
      " ['-1.56610' '0' '0.401941' '0.432793' '-0.47367' '0.420730' '2']\n",
      " ['0.827377' '1' '0.401941' '-0.47454' '-0.47367' '-0.48633' '2']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:444: DataConversionWarning: Data with input dtype <U8 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# read the data\n",
    "x_train, y_train = load_Titanic()\n",
    "\n",
    "# preprocessing: standardize numeric, encode categorical\n",
    "x_train[:,[0,2,3,4,5]] = scale(x_train[:,[0,2,3,4,5]])\n",
    "for i in [1,6]:\n",
    "    x_train[:,i] = LabelEncoder().fit_transform(x_train[:,i])\n",
    "\n",
    "# see if it works well\n",
    "print(x_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use random search to fine-tune the hyperparameters of kNN and get the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:  2.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, error_score='raise',\n",
       "          estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform'),\n",
       "          fit_params=None, iid=True, n_iter=100, n_jobs=1,\n",
       "          param_distributions={'leaf_size': array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26,\n",
       "       27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43,\n",
       "       44, 45, 46, 47, 48, 50]), 'n_neighbors': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15]), 'p': array([1, 2, 3, 4, 5])},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score=True, scoring=None, verbose=1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set the range of hyperparameters\n",
    "param_distributions = {'n_neighbors': np.array(np.linspace(1,15,15), dtype=np.int),\n",
    "                       'p': np.array(np.linspace(1,5,5), dtype=np.int),\n",
    "                       'leaf_size': np.array(np.linspace(10,50,40), dtype=np.int)}\n",
    "# initialize the random search\n",
    "random_search = RandomizedSearchCV(estimator=KNeighborsClassifier(),\n",
    "                                   param_distributions=param_distributions,\n",
    "                                   n_iter=30,\n",
    "                                   cv=10,\n",
    "                                   verbose=1)\n",
    "# start searching\n",
    "random_search.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the searching, we see the hyperparameters and accuracy of the best model, and keep the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'leaf_size': 24, 'n_neighbors': 6, 'p': 1}\n",
      "Best accuracy:  0.824915824916\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=24, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=6, p=1,\n",
      "           weights='uniform')\n"
     ]
    }
   ],
   "source": [
    "print(random_search.best_params_)\n",
    "print('Best accuracy: ', random_search.best_score_)\n",
    "print(random_search.best_estimator_)\n",
    "nn = random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there's no hyperparameters for Gaussian Naive Bayes, there's no need to do random search, so we just fit the data, and see the training accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy:  0.792368125701\n"
     ]
    }
   ],
   "source": [
    "nb = GaussianNB()\n",
    "nb.fit(x_train.astype(np.float), y_train.astype(np.float))\n",
    "print('Training accuracy: ', nb.score(x_train.astype(np.float), y_train.astype(np.float)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although Random Forest has been run on R with 0.78 test accuracy, we train here again with hyperparameters fine-tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:  4.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, error_score='raise',\n",
       "          estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=30, n_jobs=1,\n",
       "          param_distributions={'min_impurity_decrease': array([ 0.     ,  0.00526,  0.01053,  0.01579,  0.02105,  0.02632,\n",
       "        0.03158,  0.03684,  0.04211,  0.04737,  0.05263,  0.05789,\n",
       "        0.06316,  0.06842,  0.07368,  0.07895,  0.08421,  0.08947,\n",
       "        0.09474,  0.1    ]), 'n_estimators': array([ ...es': array([1, 2, 3, 4, 5, 6, 7]), 'min_samples_split': array([ 2,  3,  4,  5,  6,  7,  8,  9, 10])},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score=True, scoring=None, verbose=1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set the range of hyperparameters\n",
    "param_distributions = {'n_estimators': np.array(np.arange(10,501), dtype=np.int),\n",
    "                       'max_features': np.array(np.linspace(1,7,7), dtype=np.int),\n",
    "                       'min_samples_split': np.array(np.linspace(2,10,9), dtype=np.int),\n",
    "                       'min_samples_leaf': np.array(np.linspace(1,10,10), dtype=np.int),\n",
    "                       'min_impurity_decrease': np.linspace(0,0.1,20)}\n",
    "# initialize the random search\n",
    "random_search = RandomizedSearchCV(estimator=RandomForestClassifier(),\n",
    "                                   param_distributions=param_distributions,\n",
    "                                   n_iter=30,\n",
    "                                   cv=10,\n",
    "                                   verbose=1)\n",
    "# start searching\n",
    "random_search.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the searching, we see the hyperparameters and accuracy of the best model, and keep the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min_impurity_decrease': 0.0, 'n_estimators': 16, 'min_samples_leaf': 4, 'max_features': 7, 'min_samples_split': 4}\n",
      "Best accuracy:  0.849607182941\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=7, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=4, min_samples_split=4,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=16, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print(random_search.best_params_)\n",
    "print('Best accuracy: ', random_search.best_score_)\n",
    "print(random_search.best_estimator_)\n",
    "rf = random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use random search to fine-tune the hyperparameters of SVM and get the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:  2.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, error_score='raise',\n",
       "          estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "          fit_params=None, iid=True, n_iter=100, n_jobs=1,\n",
       "          param_distributions={'gamma': array([ 0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9,  1. ]), 'C': array([ 0.1    ,  0.35789,  0.61579,  0.87368,  1.13158,  1.38947,\n",
       "        1.64737,  1.90526,  2.16316,  2.42105,  2.67895,  2.93684,\n",
       "        3.19474,  3.45263,  3.71053,  3.96842,  4.22632,  4.48421,\n",
       "        4.74211,  5.     ]), 'tol': array([ 0.0001,  0.0012,  0.0023,  0.0034,  0.0045,  0.0056,  0.0067,\n",
       "        0.0078,  0.0089,  0.01  ])},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score=True, scoring=None, verbose=1)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set the range of hyperparameters\n",
    "param_distributions = {'C': np.linspace(0.1,5,20),\n",
    "                       'gamma': np.linspace(0.1,1,10),\n",
    "                       'tol': np.linspace(1e-4,1e-2,10)}\n",
    "# initialize the random search\n",
    "random_search = RandomizedSearchCV(estimator=SVC(),\n",
    "                                   param_distributions=param_distributions,\n",
    "                                   n_iter=100,\n",
    "                                   cv=10,\n",
    "                                   verbose=1)\n",
    "# start searching\n",
    "random_search.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the searching, we see the hyperparameters and accuracy of the best model, and keep the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.10000000000000001, 'tol': 0.01, 'C': 2.9368421052631581}\n",
      "Best accuracy:  0.829405162738\n",
      "SVC(C=2.9368421052631581, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.10000000000000001,\n",
      "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
      "  shrinking=True, tol=0.01, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "print(random_search.best_params_)\n",
    "print('Best accuracy: ', random_search.best_score_)\n",
    "print(random_search.best_estimator_)\n",
    "svm = random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network using Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have more flexibility, we use Keras instead of Multilayer Perceptron in scikit_learn.\n",
    "\n",
    "In concern with model selection, since the data only has 7 features and is not time series, both CNN and RNN are not quite suitable, therefore only MLP(Multilayer Perceptron) is appropriate.\n",
    "\n",
    "First, define the mlp model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mlp_model(hidden_dims, dropout=0.5, lr=0.001):\n",
    "    # build the model\n",
    "    mlp = Sequential()\n",
    "    mlp.add(Flatten(input_shape=(7,1)))\n",
    "    for hidden_dim in hidden_dims:\n",
    "        mlp.add(Dense(units=hidden_dim, activation='relu'))\n",
    "        if dropout:\n",
    "            mlp.add(Dropout(rate=dropout))\n",
    "    mlp.add(Dense(units=2, activation='softmax'))\n",
    "    \n",
    "    # set the optimizer and loss\n",
    "    rmsprop = RMSprop(lr=lr, decay=0.99)\n",
    "    mlp.compile(loss='sparse_categorical_crossentropy',\n",
    "                       optimizer=rmsprop, metrics=['accuracy'])\n",
    "    return mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then set the early stopping and train the model. 91 observations are used for monitoring, and 800 observations are used to train. Here we just try usual hyperparameters to train and see the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 91 samples\n",
      "Epoch 1/100\n",
      "800/800 [==============================] - 0s - loss: 0.6687 - acc: 0.6238 - val_loss: 0.6526 - val_acc: 0.6264\n",
      "Epoch 2/100\n",
      "800/800 [==============================] - 0s - loss: 0.6686 - acc: 0.6238 - val_loss: 0.6505 - val_acc: 0.6264\n",
      "Epoch 3/100\n",
      "800/800 [==============================] - 0s - loss: 0.6725 - acc: 0.6212 - val_loss: 0.6493 - val_acc: 0.6264\n",
      "Epoch 4/100\n",
      "800/800 [==============================] - 0s - loss: 0.6782 - acc: 0.5975 - val_loss: 0.6483 - val_acc: 0.6264\n",
      "Epoch 5/100\n",
      "800/800 [==============================] - 0s - loss: 0.6743 - acc: 0.6075 - val_loss: 0.6477 - val_acc: 0.6264\n",
      "Epoch 6/100\n",
      "800/800 [==============================] - 0s - loss: 0.6659 - acc: 0.6288 - val_loss: 0.6471 - val_acc: 0.6264\n",
      "Epoch 7/100\n",
      "800/800 [==============================] - 0s - loss: 0.6724 - acc: 0.6075 - val_loss: 0.6466 - val_acc: 0.6264\n",
      "Epoch 8/100\n",
      "800/800 [==============================] - 0s - loss: 0.6787 - acc: 0.6112 - val_loss: 0.6461 - val_acc: 0.6264\n",
      "Epoch 9/100\n",
      "800/800 [==============================] - 0s - loss: 0.6677 - acc: 0.6163 - val_loss: 0.6457 - val_acc: 0.6264\n",
      "Epoch 10/100\n",
      "800/800 [==============================] - 0s - loss: 0.6602 - acc: 0.6313 - val_loss: 0.6454 - val_acc: 0.6264\n",
      "Epoch 11/100\n",
      "800/800 [==============================] - 0s - loss: 0.6835 - acc: 0.6050 - val_loss: 0.6451 - val_acc: 0.6264\n",
      "Epoch 12/100\n",
      "800/800 [==============================] - 0s - loss: 0.6792 - acc: 0.6025 - val_loss: 0.6448 - val_acc: 0.6264\n",
      "Epoch 13/100\n",
      "800/800 [==============================] - 0s - loss: 0.6735 - acc: 0.6262 - val_loss: 0.6446 - val_acc: 0.6264\n",
      "Epoch 14/100\n",
      "800/800 [==============================] - 0s - loss: 0.6667 - acc: 0.6200 - val_loss: 0.6443 - val_acc: 0.6264\n",
      "Epoch 15/100\n",
      "800/800 [==============================] - 0s - loss: 0.6721 - acc: 0.6200 - val_loss: 0.6441 - val_acc: 0.6264\n",
      "Epoch 16/100\n",
      "800/800 [==============================] - 0s - loss: 0.6761 - acc: 0.6375 - val_loss: 0.6439 - val_acc: 0.6264\n",
      "Epoch 17/100\n",
      "800/800 [==============================] - 0s - loss: 0.6746 - acc: 0.6337 - val_loss: 0.6437 - val_acc: 0.6264\n",
      "Epoch 18/100\n",
      "800/800 [==============================] - 0s - loss: 0.6592 - acc: 0.6238 - val_loss: 0.6436 - val_acc: 0.6264\n",
      "Epoch 19/100\n",
      "800/800 [==============================] - 0s - loss: 0.6609 - acc: 0.6175 - val_loss: 0.6434 - val_acc: 0.6264\n",
      "Epoch 20/100\n",
      "800/800 [==============================] - 0s - loss: 0.6591 - acc: 0.6337 - val_loss: 0.6432 - val_acc: 0.6264\n",
      "Epoch 21/100\n",
      "800/800 [==============================] - 0s - loss: 0.6566 - acc: 0.6375 - val_loss: 0.6431 - val_acc: 0.6264\n",
      "Epoch 22/100\n",
      "800/800 [==============================] - 0s - loss: 0.6709 - acc: 0.6125 - val_loss: 0.6429 - val_acc: 0.6264\n",
      "Epoch 23/100\n",
      "800/800 [==============================] - 0s - loss: 0.6626 - acc: 0.6225 - val_loss: 0.6428 - val_acc: 0.6264\n",
      "Epoch 24/100\n",
      "800/800 [==============================] - 0s - loss: 0.6627 - acc: 0.6187 - val_loss: 0.6426 - val_acc: 0.6264\n",
      "Epoch 25/100\n",
      "800/800 [==============================] - 0s - loss: 0.6700 - acc: 0.5988 - val_loss: 0.6425 - val_acc: 0.6264\n",
      "Epoch 26/100\n",
      "800/800 [==============================] - 0s - loss: 0.6746 - acc: 0.6163 - val_loss: 0.6424 - val_acc: 0.6264\n",
      "Epoch 27/100\n",
      "800/800 [==============================] - 0s - loss: 0.6658 - acc: 0.6112 - val_loss: 0.6423 - val_acc: 0.6264\n",
      "Epoch 28/100\n",
      "800/800 [==============================] - 0s - loss: 0.6622 - acc: 0.6225 - val_loss: 0.6422 - val_acc: 0.6264\n",
      "Epoch 29/100\n",
      "800/800 [==============================] - 0s - loss: 0.6640 - acc: 0.6200 - val_loss: 0.6420 - val_acc: 0.6264\n",
      "Epoch 30/100\n",
      "800/800 [==============================] - 0s - loss: 0.6669 - acc: 0.6138 - val_loss: 0.6419 - val_acc: 0.6264\n",
      "Epoch 31/100\n",
      "800/800 [==============================] - 0s - loss: 0.6602 - acc: 0.6187 - val_loss: 0.6418 - val_acc: 0.6264\n",
      "Epoch 32/100\n",
      "800/800 [==============================] - 0s - loss: 0.6516 - acc: 0.6238 - val_loss: 0.6417 - val_acc: 0.6264\n",
      "Epoch 33/100\n",
      "800/800 [==============================] - 0s - loss: 0.6717 - acc: 0.6225 - val_loss: 0.6416 - val_acc: 0.6264\n",
      "Epoch 34/100\n",
      "800/800 [==============================] - 0s - loss: 0.6632 - acc: 0.6250 - val_loss: 0.6415 - val_acc: 0.6264\n",
      "Epoch 35/100\n",
      "800/800 [==============================] - 0s - loss: 0.6591 - acc: 0.6212 - val_loss: 0.6415 - val_acc: 0.6264\n",
      "Epoch 36/100\n",
      "800/800 [==============================] - 0s - loss: 0.6634 - acc: 0.6100 - val_loss: 0.6414 - val_acc: 0.6264\n",
      "Epoch 37/100\n",
      "800/800 [==============================] - 0s - loss: 0.6731 - acc: 0.6225 - val_loss: 0.6413 - val_acc: 0.6264\n",
      "Epoch 38/100\n",
      "800/800 [==============================] - 0s - loss: 0.6692 - acc: 0.6187 - val_loss: 0.6412 - val_acc: 0.6264\n",
      "Epoch 39/100\n",
      "800/800 [==============================] - 0s - loss: 0.6658 - acc: 0.6125 - val_loss: 0.6411 - val_acc: 0.6264\n",
      "Epoch 40/100\n",
      "800/800 [==============================] - 0s - loss: 0.6723 - acc: 0.6038 - val_loss: 0.6410 - val_acc: 0.6264\n",
      "Epoch 41/100\n",
      "800/800 [==============================] - 0s - loss: 0.6720 - acc: 0.6163 - val_loss: 0.6410 - val_acc: 0.6264\n",
      "Epoch 42/100\n",
      "800/800 [==============================] - 0s - loss: 0.6594 - acc: 0.6225 - val_loss: 0.6409 - val_acc: 0.6264\n",
      "Epoch 43/100\n",
      "800/800 [==============================] - 0s - loss: 0.6641 - acc: 0.6262 - val_loss: 0.6408 - val_acc: 0.6264\n",
      "Epoch 44/100\n",
      "800/800 [==============================] - 0s - loss: 0.6553 - acc: 0.6212 - val_loss: 0.6407 - val_acc: 0.6264\n",
      "Epoch 45/100\n",
      "800/800 [==============================] - 0s - loss: 0.6663 - acc: 0.6238 - val_loss: 0.6407 - val_acc: 0.6264\n",
      "Epoch 46/100\n",
      "800/800 [==============================] - 0s - loss: 0.6535 - acc: 0.6412 - val_loss: 0.6406 - val_acc: 0.6264\n",
      "Epoch 47/100\n",
      "800/800 [==============================] - 0s - loss: 0.6624 - acc: 0.6300 - val_loss: 0.6405 - val_acc: 0.6264\n",
      "Epoch 48/100\n",
      "800/800 [==============================] - 0s - loss: 0.6706 - acc: 0.6088 - val_loss: 0.6405 - val_acc: 0.6264\n",
      "Epoch 49/100\n",
      "800/800 [==============================] - 0s - loss: 0.6731 - acc: 0.6175 - val_loss: 0.6404 - val_acc: 0.6264\n",
      "Epoch 50/100\n",
      "800/800 [==============================] - 0s - loss: 0.6450 - acc: 0.6350 - val_loss: 0.6403 - val_acc: 0.6264\n",
      "Epoch 51/100\n",
      "800/800 [==============================] - 0s - loss: 0.6616 - acc: 0.6250 - val_loss: 0.6403 - val_acc: 0.6264\n",
      "Epoch 52/100\n",
      "800/800 [==============================] - 0s - loss: 0.6669 - acc: 0.6212 - val_loss: 0.6402 - val_acc: 0.6264\n",
      "Epoch 53/100\n",
      "800/800 [==============================] - 0s - loss: 0.6659 - acc: 0.6313 - val_loss: 0.6402 - val_acc: 0.6264\n",
      "Epoch 54/100\n",
      "800/800 [==============================] - 0s - loss: 0.6687 - acc: 0.6225 - val_loss: 0.6401 - val_acc: 0.6264\n",
      "Epoch 55/100\n",
      "800/800 [==============================] - 0s - loss: 0.6676 - acc: 0.6187 - val_loss: 0.6400 - val_acc: 0.6264\n",
      "Epoch 56/100\n",
      "800/800 [==============================] - 0s - loss: 0.6640 - acc: 0.6313 - val_loss: 0.6400 - val_acc: 0.6264\n",
      "Epoch 57/100\n",
      "800/800 [==============================] - 0s - loss: 0.6640 - acc: 0.6200 - val_loss: 0.6399 - val_acc: 0.6264\n",
      "Epoch 58/100\n",
      "800/800 [==============================] - 0s - loss: 0.6571 - acc: 0.6125 - val_loss: 0.6399 - val_acc: 0.6264\n",
      "Epoch 59/100\n",
      "800/800 [==============================] - 0s - loss: 0.6574 - acc: 0.6300 - val_loss: 0.6398 - val_acc: 0.6264\n",
      "Epoch 60/100\n",
      "800/800 [==============================] - 0s - loss: 0.6559 - acc: 0.6200 - val_loss: 0.6398 - val_acc: 0.6264\n",
      "Epoch 61/100\n",
      "800/800 [==============================] - 0s - loss: 0.6672 - acc: 0.6088 - val_loss: 0.6397 - val_acc: 0.6264\n",
      "Epoch 62/100\n",
      "800/800 [==============================] - 0s - loss: 0.6571 - acc: 0.6175 - val_loss: 0.6397 - val_acc: 0.6264\n",
      "Epoch 63/100\n",
      "800/800 [==============================] - 0s - loss: 0.6637 - acc: 0.6062 - val_loss: 0.6396 - val_acc: 0.6264\n",
      "Epoch 64/100\n",
      "800/800 [==============================] - 0s - loss: 0.6576 - acc: 0.6187 - val_loss: 0.6396 - val_acc: 0.6264\n",
      "Epoch 65/100\n",
      "800/800 [==============================] - 0s - loss: 0.6620 - acc: 0.6150 - val_loss: 0.6395 - val_acc: 0.6264\n",
      "Epoch 66/100\n",
      "800/800 [==============================] - 0s - loss: 0.6755 - acc: 0.5988 - val_loss: 0.6395 - val_acc: 0.6264\n",
      "Epoch 67/100\n",
      "800/800 [==============================] - 0s - loss: 0.6611 - acc: 0.6275 - val_loss: 0.6394 - val_acc: 0.6264\n",
      "Epoch 68/100\n",
      "800/800 [==============================] - 0s - loss: 0.6565 - acc: 0.6362 - val_loss: 0.6394 - val_acc: 0.6264\n",
      "Epoch 69/100\n",
      "800/800 [==============================] - 0s - loss: 0.6664 - acc: 0.6238 - val_loss: 0.6393 - val_acc: 0.6264\n",
      "Epoch 70/100\n",
      "800/800 [==============================] - 0s - loss: 0.6571 - acc: 0.6313 - val_loss: 0.6393 - val_acc: 0.6264\n",
      "Epoch 71/100\n",
      "800/800 [==============================] - 0s - loss: 0.6592 - acc: 0.6125 - val_loss: 0.6392 - val_acc: 0.6264\n",
      "Epoch 72/100\n",
      "800/800 [==============================] - 0s - loss: 0.6624 - acc: 0.6225 - val_loss: 0.6392 - val_acc: 0.6264\n",
      "Epoch 73/100\n",
      "800/800 [==============================] - 0s - loss: 0.6474 - acc: 0.6438 - val_loss: 0.6392 - val_acc: 0.6264\n",
      "Epoch 74/100\n",
      "800/800 [==============================] - 0s - loss: 0.6694 - acc: 0.6187 - val_loss: 0.6391 - val_acc: 0.6264\n",
      "Epoch 75/100\n",
      "800/800 [==============================] - 0s - loss: 0.6564 - acc: 0.6250 - val_loss: 0.6391 - val_acc: 0.6264\n",
      "Epoch 76/100\n",
      "800/800 [==============================] - 0s - loss: 0.6685 - acc: 0.6212 - val_loss: 0.6390 - val_acc: 0.6264\n",
      "Epoch 77/100\n",
      "800/800 [==============================] - 0s - loss: 0.6691 - acc: 0.6275 - val_loss: 0.6390 - val_acc: 0.6264\n",
      "Epoch 78/100\n",
      "800/800 [==============================] - 0s - loss: 0.6595 - acc: 0.6100 - val_loss: 0.6390 - val_acc: 0.6264\n",
      "Epoch 79/100\n",
      "800/800 [==============================] - 0s - loss: 0.6508 - acc: 0.6225 - val_loss: 0.6389 - val_acc: 0.6264\n",
      "Epoch 80/100\n",
      "800/800 [==============================] - 0s - loss: 0.6718 - acc: 0.6088 - val_loss: 0.6389 - val_acc: 0.6264\n",
      "Epoch 81/100\n",
      "800/800 [==============================] - 0s - loss: 0.6549 - acc: 0.6200 - val_loss: 0.6388 - val_acc: 0.6264\n",
      "Epoch 82/100\n",
      "800/800 [==============================] - 0s - loss: 0.6538 - acc: 0.6337 - val_loss: 0.6388 - val_acc: 0.6264\n",
      "Epoch 83/100\n",
      "800/800 [==============================] - 0s - loss: 0.6643 - acc: 0.6212 - val_loss: 0.6388 - val_acc: 0.6264\n",
      "Epoch 84/100\n",
      "800/800 [==============================] - 0s - loss: 0.6722 - acc: 0.6238 - val_loss: 0.6387 - val_acc: 0.6264\n",
      "Epoch 85/100\n",
      "800/800 [==============================] - 0s - loss: 0.6646 - acc: 0.6200 - val_loss: 0.6387 - val_acc: 0.6264\n",
      "Epoch 86/100\n",
      "800/800 [==============================] - 0s - loss: 0.6576 - acc: 0.6275 - val_loss: 0.6386 - val_acc: 0.6264\n",
      "Epoch 87/100\n",
      "800/800 [==============================] - 0s - loss: 0.6624 - acc: 0.6212 - val_loss: 0.6386 - val_acc: 0.6264\n",
      "Epoch 88/100\n",
      "800/800 [==============================] - 0s - loss: 0.6710 - acc: 0.6150 - val_loss: 0.6386 - val_acc: 0.6264\n",
      "Epoch 89/100\n",
      "800/800 [==============================] - 0s - loss: 0.6675 - acc: 0.6200 - val_loss: 0.6385 - val_acc: 0.6264\n",
      "Epoch 90/100\n",
      "800/800 [==============================] - 0s - loss: 0.6572 - acc: 0.6313 - val_loss: 0.6385 - val_acc: 0.6264\n",
      "Epoch 91/100\n",
      "800/800 [==============================] - 0s - loss: 0.6511 - acc: 0.6337 - val_loss: 0.6385 - val_acc: 0.6264\n",
      "Epoch 92/100\n",
      "800/800 [==============================] - 0s - loss: 0.6536 - acc: 0.6238 - val_loss: 0.6384 - val_acc: 0.6264\n",
      "Epoch 93/100\n",
      "800/800 [==============================] - 0s - loss: 0.6650 - acc: 0.6125 - val_loss: 0.6384 - val_acc: 0.6264\n",
      "Epoch 94/100\n",
      "800/800 [==============================] - 0s - loss: 0.6519 - acc: 0.6412 - val_loss: 0.6384 - val_acc: 0.6264\n",
      "Epoch 95/100\n",
      "800/800 [==============================] - 0s - loss: 0.6495 - acc: 0.6200 - val_loss: 0.6383 - val_acc: 0.6264\n",
      "Epoch 96/100\n",
      "800/800 [==============================] - 0s - loss: 0.6652 - acc: 0.6362 - val_loss: 0.6383 - val_acc: 0.6264\n",
      "Epoch 97/100\n",
      "800/800 [==============================] - 0s - loss: 0.6502 - acc: 0.6088 - val_loss: 0.6383 - val_acc: 0.6264\n",
      "Epoch 98/100\n",
      "800/800 [==============================] - 0s - loss: 0.6602 - acc: 0.6400 - val_loss: 0.6382 - val_acc: 0.6264\n",
      "Epoch 99/100\n",
      "800/800 [==============================] - 0s - loss: 0.6646 - acc: 0.6125 - val_loss: 0.6382 - val_acc: 0.6264\n",
      "Epoch 100/100\n",
      "800/800 [==============================] - 0s - loss: 0.6598 - acc: 0.6262 - val_loss: 0.6382 - val_acc: 0.6264\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbd7346d940>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earlystop = EarlyStopping(min_delta=0.0001, patience=5)\n",
    "\n",
    "# use test data to monitor early stopping\n",
    "model = mlp_model(hidden_dims=[50,25],\n",
    "                  dropout=0.2,\n",
    "                  lr=0.001)\n",
    "model.fit(x_train[:800].reshape(-1,7,1), y_train[:800].reshape(-1,1),\n",
    "               batch_size=32,\n",
    "               epochs=100,\n",
    "               validation_data=(x_train[800:].reshape(-1,7,1), y_train[800:].reshape(-1,1)),\n",
    "               callbacks=[earlystop],\n",
    "               initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The result above is actually not satisfied, so we use random search to find the best set of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_parameters={'lr': 0.001, 'dropout': 0.5, 'hidden_dims': []}\n",
    "lowest_err = 1000\n",
    "lr_range = (0.1,0.0001); dropout_range = (0.3,0.8); dense_range = (512,2048)\n",
    "while True:\n",
    "    lr = np.random.uniform(lr_range[0], lr_range[1])\n",
    "    dropout = np.random.uniform(dropout_range[0], dropout_range[1])\n",
    "    dense_dim = int(np.random.uniform(dense_range[0], dense_range[1]))\n",
    "    drop_conv, avgpool = np.random.binomial(1,0.5,2)\n",
    "    ktf.clear_session()\n",
    "    test_err, test_acc = simpleCNN_model(lr, dropout, dense_dim, drop_conv, avgpool)\n",
    "    if test_err < lowest_err:\n",
    "        print('new lowest: ', round(test_err,2), round(test_acc,2), \n",
    "              (round(lr,4), round(dropout,2), dense_dim, bool(drop_conv), bool(avgpool)))\n",
    "        lowest_err = test_err\n",
    "        best_parameters['lr'] = lr\n",
    "        best_parameters['dropout'] = dropout\n",
    "        best_parameters['dense_dim'] = dense_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is used to generate submission file for Kaggle competition using trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\chase\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\utils\\validation.py:444: DataConversionWarning: Data with input dtype <U8 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# read the test data and preprocess\n",
    "x_test = load_Titanic(filename='../data/test - processed.csv', test=True)\n",
    "x_test[:,[0,2,3,4,5]] = scale(x_test[:,[0,2,3,4,5]])\n",
    "for i in [1,6]:\n",
    "    x_test[:,i] = LabelEncoder().fit_transform(x_test[:,i])\n",
    "    \n",
    "# create submission file\n",
    "create_submission(svm, x_test, '../submission/submission_svm_ft.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
