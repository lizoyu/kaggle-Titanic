{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Survival Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is intended to predict survival of Titanic passengers using Python and its libraries. Many models will be covered with fine-tuned hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom keras.layers import Input, Dense, Flatten, Activation\\nfrom keras.layers.core import Dropout\\nfrom keras.layers.normalization import BatchNormalization\\nfrom keras.optimizers import RMSprop\\nfrom keras.models import Model, Sequential, load_model\\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\\nfrom keras import backend as ktf\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from lib.data_utils import load_Titanic, create_submission\n",
    "# scikit_learn\n",
    "from sklearn.preprocessing import scale, LabelEncoder\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, Lasso\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "# Keras\n",
    "'''\n",
    "from keras.layers import Input, Dense, Flatten, Activation\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras import backend as ktf\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['0.8273772438659699' '1' '-0.221936632898316' '-0.879740569393426' '2'\n",
      "  '0.3248226020114721' '2']\n",
      " ['-1.566106925889157' '0' '-0.366023089336598' '1.3612199269233827' '0'\n",
      "  '0.3248226020114721' '3']\n",
      " ['0.8273772438659699' '0' '-0.273967853278807' '-0.798539974052804' '2'\n",
      "  '-0.682686962816101' '1']\n",
      " ['-1.566106925889157' '0' '-0.348762732575762' '1.0620380556287148' '2'\n",
      "  '0.3248226020114721' '3']\n",
      " ['0.8273772438659699' '1' '-0.348762732575762' '-0.784179243007400' '2'\n",
      "  '-0.682686962816101' '2']]\n",
      "[['0.873481905063612' '1' '-0.272216068494200' '-0.866889649115075' '1'\n",
      "  '-0.686792910356192' '2']\n",
      " ['0.873481905063612' '0' '-0.360028904313975' '-0.969003971902478' '2'\n",
      "  '0.3761220785579247' '3']\n",
      " ['-0.315819190430165' '1' '-0.419407107582584' '-0.669117059798610' '1'\n",
      "  '-0.686792910356192' '2']\n",
      " ['0.873481905063612' '1' '-0.181894294508146' '-0.773508803025895' '2'\n",
      "  '-0.686792910356192' '2']\n",
      " ['0.873481905063612' '0' '-0.088954498087713' '-0.443659387190805' '2'\n",
      "  '1.1302722649391934' '3']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\chase\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\utils\\validation.py:444: DataConversionWarning: Data with input dtype <U18 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# read the data\n",
    "x_train, y_train, x_test = load_Titanic('../data/all_tf.csv')\n",
    "\n",
    "# preprocessing: standardize numeric, encode categorical\n",
    "x_train[:,[0,2,3,5]] = scale(x_train[:,[0,2,3,5]])\n",
    "x_test[:,[0,2,3,5]] = scale(x_test[:,[0,2,3,5]])\n",
    "for i in [1,4,6]:\n",
    "    x_train[:,i] = LabelEncoder().fit_transform(x_train[:,i])\n",
    "    x_test[:,i] = LabelEncoder().fit_transform(x_test[:,i])\n",
    "\n",
    "# see if it works well\n",
    "print(x_train[:5])\n",
    "print(x_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use random search to fine-tune the hyperparameters of logistic model and get the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 2000 out of 2000 | elapsed:   26.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, error_score='raise',\n",
       "          estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=200, n_jobs=1,\n",
       "          param_distributions={'penalty': ['l1', 'l2'], 'tol': array([  1.00000e-05,   1.20000e-04,   2.30000e-04,   3.40000e-04,\n",
       "         4.50000e-04,   5.60000e-04,   6.70000e-04,   7.80000e-04,\n",
       "         8.90000e-04,   1.00000e-03]), 'C': array([ 0.1    ,  0.31111,  0.52222,  0.73333,  0.94444,  1.15556,\n",
       "        1.36667,  1.57778,  1.78889,  2.     ])},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score=True, scoring=None, verbose=1)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set the range of hyperparameters\n",
    "param_distributions = {'penalty': ['l1', 'l2'],\n",
    "                       'tol': np.linspace(1e-5,1e-3,10),\n",
    "                       'C': np.linspace(0.1,2,10)}\n",
    "# initialize the random search\n",
    "random_search = RandomizedSearchCV(estimator=LogisticRegression(),\n",
    "                                   param_distributions=param_distributions,\n",
    "                                   n_iter=30,\n",
    "                                   cv=10,\n",
    "                                   verbose=1)\n",
    "# start searching\n",
    "random_search.fit(x_train.astype(np.float64), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the searching, we see the hyperparameters and accuracy of the best model, and keep the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'penalty': 'l1', 'tol': 1.0000000000000001e-05, 'C': 0.31111111111111112}\n",
      "Best accuracy:  0.7822671156\n",
      "LogisticRegression(C=0.31111111111111112, class_weight=None, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l1', random_state=None,\n",
      "          solver='liblinear', tol=1.0000000000000001e-05, verbose=0,\n",
      "          warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print(random_search.best_params_)\n",
    "print('Best accuracy: ', random_search.best_score_)\n",
    "print(random_search.best_estimator_)\n",
    "logis = random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  1.  0.  1.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(logis.predict(x_test[:10].astype(np.float)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:    1.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, error_score='raise',\n",
       "          estimator=ElasticNet(alpha=1.0, copy_X=True, fit_intercept=False, l1_ratio=0.5,\n",
       "      max_iter=10000, normalize=False, positive=False, precompute=False,\n",
       "      random_state=None, selection='cyclic', tol=0.0001, warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=30, n_jobs=1,\n",
       "          param_distributions={'tol': array([  1.00000e-05,   1.20000e-04,   2.30000e-04,   3.40000e-04,\n",
       "         4.50000e-04,   5.60000e-04,   6.70000e-04,   7.80000e-04,\n",
       "         8.90000e-04,   1.00000e-03]), 'alpha': array([  1.     ,   1.09091, ...,   9.90909,  10.     ])},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score=True, scoring=None, verbose=1)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set the range of hyperparameters\n",
    "param_distributions = {'alpha': np.linspace(1,10,100),\n",
    "                       'tol': np.linspace(1e-5,1e-3,10)}\n",
    "# initialize the random search\n",
    "random_search = RandomizedSearchCV(estimator=ElasticNet(fit_intercept=False, max_iter=10000),\n",
    "                                   param_distributions=param_distributions,\n",
    "                                   n_iter=30,\n",
    "                                   cv=10,\n",
    "                                   verbose=1)\n",
    "# start searching\n",
    "random_search.fit(x_train.astype(np.float), y_train.astype(np.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 1.0, 'tol': 0.00034000000000000002}\n",
      "Best accuracy:  -0.413028041049\n",
      "ElasticNet(alpha=1.0, copy_X=True, fit_intercept=False, l1_ratio=0.5,\n",
      "      max_iter=10000, normalize=False, positive=False, precompute=False,\n",
      "      random_state=None, selection='cyclic', tol=0.00034000000000000002,\n",
      "      warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print(random_search.best_params_)\n",
    "print('Best accuracy: ', random_search.best_score_)\n",
    "print(random_search.best_estimator_)\n",
    "enet = random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.1559616 ,  0.2483576 ,  0.15601073,  0.18465058,  0.24848808,\n",
       "        0.18466514,  0.09220014,  0.18494196,  0.19103329,  0.1848966 ,\n",
       "        0.18462932,  0.18491486,  0.24896007,  0.18491486,  0.24888495,\n",
       "        0.19135475,  0.15606793,  0.12727759,  0.12087459,  0.19103316,\n",
       "        0.12779036,  0.05692337,  0.24871956,  0.12779865,  0.19192465,\n",
       "        0.18477212,  0.06404554,  0.12727759,  0.1849545 ,  0.12753844,\n",
       "        0.18491486,  0.18496254,  0.24861274,  0.24864492,  0.12778323,\n",
       "        0.12727772,  0.12087817,  0.12089501,  0.18467196,  0.18510926,\n",
       "        0.12742192,  0.18492005,  0.18462799,  0.24850152,  0.24884656,\n",
       "        0.18463016,  0.12761631,  0.15595929,  0.19160935,  0.24854993])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enet.predict(x_test.astype(np.float)[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use random search to fine-tune the hyperparameters of kNN and get the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:   48.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, error_score='raise',\n",
       "          estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform'),\n",
       "          fit_params=None, iid=True, n_iter=30, n_jobs=1,\n",
       "          param_distributions={'leaf_size': array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26,\n",
       "       27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43,\n",
       "       44, 45, 46, 47, 48, 50]), 'p': array([1, 2, 3, 4, 5]), 'n_neighbors': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15])},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score=True, scoring=None, verbose=1)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set the range of hyperparameters\n",
    "param_distributions = {'n_neighbors': np.array(np.linspace(1,15,15), dtype=np.int),\n",
    "                       'p': np.array(np.linspace(1,5,5), dtype=np.int),\n",
    "                       'leaf_size': np.array(np.linspace(10,50,40), dtype=np.int)}\n",
    "# initialize the random search\n",
    "random_search = RandomizedSearchCV(estimator=KNeighborsClassifier(),\n",
    "                                   param_distributions=param_distributions,\n",
    "                                   n_iter=30,\n",
    "                                   cv=10,\n",
    "                                   verbose=1)\n",
    "# start searching\n",
    "random_search.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the searching, we see the hyperparameters and accuracy of the best model, and keep the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'leaf_size': 50, 'p': 2, 'n_neighbors': 9}\n",
      "Best accuracy:  0.83164983165\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=50, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=9, p=2,\n",
      "           weights='uniform')\n"
     ]
    }
   ],
   "source": [
    "print(random_search.best_params_)\n",
    "print('Best accuracy: ', random_search.best_score_)\n",
    "print(random_search.best_estimator_)\n",
    "knn = random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the model parameters\n",
    "knn = KNeighborsClassifier(algorithm='auto', leaf_size=50, metric='minkowski',\n",
    "           metric_params=None, n_jobs=1, n_neighbors=9, p=2,\n",
    "           weights='uniform')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there's no hyperparameters for Gaussian Naive Bayes, there's no need to do random search, so we just fit the data, and see the training accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy:  0.768799102132\n"
     ]
    }
   ],
   "source": [
    "nb = GaussianNB()\n",
    "nb.fit(x_train.astype(np.float), y_train.astype(np.float))\n",
    "print('Training accuracy: ', nb.score(x_train.astype(np.float), y_train.astype(np.float)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although Random Forest has been run on R with 0.78 test accuracy, we train here again with hyperparameters fine-tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed: 27.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, error_score='raise',\n",
       "          estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=100, n_jobs=1,\n",
       "          param_distributions={'min_impurity_decrease': array([ 0.     ,  0.00526,  0.01053,  0.01579,  0.02105,  0.02632,\n",
       "        0.03158,  0.03684,  0.04211,  0.04737,  0.05263,  0.05789,\n",
       "        0.06316,  0.06842,  0.07368,  0.07895,  0.08421,  0.08947,\n",
       "        0.09474,  0.1    ]), 'n_estimators': array([ ...s_split': array([ 2,  3,  4,  5,  6,  7,  8,  9, 10]), 'max_features': array([1, 2, 3, 4, 5, 6, 7])},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score=True, scoring=None, verbose=1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set the range of hyperparameters\n",
    "param_distributions = {'n_estimators': np.array(np.arange(10,501), dtype=np.int),\n",
    "                       'max_features': np.array(np.linspace(1,7,7), dtype=np.int),\n",
    "                       'min_samples_split': np.array(np.linspace(2,10,9), dtype=np.int),\n",
    "                       'min_samples_leaf': np.array(np.linspace(1,10,10), dtype=np.int),\n",
    "                       'min_impurity_decrease': np.linspace(0,0.1,20)}\n",
    "# initialize the random search\n",
    "random_search = RandomizedSearchCV(estimator=RandomForestClassifier(),\n",
    "                                   param_distributions=param_distributions,\n",
    "                                   n_iter=30,\n",
    "                                   cv=10,\n",
    "                                   verbose=1)\n",
    "# start searching\n",
    "random_search.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the searching, we see the hyperparameters and accuracy of the best model, and keep the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min_impurity_decrease': 0.0, 'n_estimators': 265, 'max_features': 5, 'min_samples_split': 2, 'min_samples_leaf': 9}\n",
      "Best accuracy:  0.838383838384\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=5, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=9, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=265, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print(random_search.best_params_)\n",
    "print('Best accuracy: ', random_search.best_score_)\n",
    "print(random_search.best_estimator_)\n",
    "rf = random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the model parameters\n",
    "rf = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=None, max_features=5, max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=9, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=265, n_jobs=1,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use random search to fine-tune the hyperparameters of SVM and get the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:  1.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, error_score='raise',\n",
       "          estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "          fit_params=None, iid=True, n_iter=30, n_jobs=1,\n",
       "          param_distributions={'C': array([ 0.1    ,  0.14737,  0.19474,  0.24211,  0.28947,  0.33684,\n",
       "        0.38421,  0.43158,  0.47895,  0.52632,  0.57368,  0.62105,\n",
       "        0.66842,  0.71579,  0.76316,  0.81053,  0.85789,  0.90526,\n",
       "        0.95263,  1.     ]), 'tol': array([ 0.0001,  0.0012,  0.0023,  0.0034,  0.0045,  0.0056,  0.0067,\n",
       "        0.0078,  0.0089,  0.01  ]), 'gamma': array([ 0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9,  1. ])},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score=True, scoring=None, verbose=1)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set the range of hyperparameters\n",
    "param_distributions = {'C': np.linspace(0.1,1,20),\n",
    "                       'gamma': np.linspace(0.1,1,10),\n",
    "                       'tol': np.linspace(1e-4,1e-2,10)}\n",
    "# initialize the random search\n",
    "random_search = RandomizedSearchCV(estimator=SVC(probability=True),\n",
    "                                   param_distributions=param_distributions,\n",
    "                                   n_iter=30,\n",
    "                                   cv=10,\n",
    "                                   verbose=1)\n",
    "# start searching\n",
    "random_search.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the searching, we see the hyperparameters and accuracy of the best model, and keep the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.90526315789473688, 'tol': 0.0067000000000000002, 'gamma': 0.59999999999999998}\n",
      "Best accuracy:  0.836139169473\n",
      "SVC(C=0.90526315789473688, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.59999999999999998,\n",
      "  kernel='rbf', max_iter=-1, probability=True, random_state=None,\n",
      "  shrinking=True, tol=0.0067000000000000002, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "print(random_search.best_params_)\n",
    "print('Best accuracy: ', random_search.best_score_)\n",
    "print(random_search.best_estimator_)\n",
    "svm = random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the model parameters\n",
    "svm = SVC(C=0.90526315789473688, cache_size=200, class_weight=None, coef0=0.0,\n",
    "  decision_function_shape='ovr', degree=3, gamma=0.59999999999999998,\n",
    "  kernel='rbf', max_iter=-1, probability=True, random_state=None,\n",
    "  shrinking=True, tol=0.0067000000000000002, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we try grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1000 candidates, totalling 3000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 3000 out of 3000 | elapsed: 12.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': array([ 0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9,  1. ]), 'gamma': array([ 0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9,  1. ]), 'tol': array([ 0.0001,  0.0012,  0.0023,  0.0034,  0.0045,  0.0056,  0.0067,\n",
       "        0.0078,  0.0089,  0.01  ])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set the range of hyperparameters\n",
    "param_distributions = {'C': np.linspace(0.1,1,10),\n",
    "                       'gamma': np.linspace(0.1,1,10),\n",
    "                       'tol': np.linspace(1e-4,1e-2,10)}\n",
    "# initialize the random search\n",
    "grid_search = GridSearchCV(estimator=SVC(probability=True),\n",
    "                           param_grid=param_distributions,\n",
    "                           cv=None,\n",
    "                           verbose=1)\n",
    "# start searching\n",
    "grid_search.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.80000000000000004, 'gamma': 0.10000000000000001, 'tol': 0.0001}\n",
      "Best accuracy:  0.829405162738\n",
      "SVC(C=0.80000000000000004, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.10000000000000001,\n",
      "  kernel='rbf', max_iter=-1, probability=True, random_state=None,\n",
      "  shrinking=True, tol=0.0001, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "print(grid_search.best_params_)\n",
    "print('Best accuracy: ', grid_search.best_score_)\n",
    "print(grid_search.best_estimator_)\n",
    "svm = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search doesn't show significant advantage on training accuracy, and it takes much longer time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP in scikit_learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:  1.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, error_score='raise',\n",
       "          estimator=MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='lbfgs', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=30, n_jobs=1,\n",
       "          param_distributions={'hidden_layer_sizes': [(50, 25), (25, 25), (25, 10), (50,), (25,), (10,)], 'tol': array([ 0.0001,  0.0012,  0.0023,  0.0034,  0.0045,  0.0056,  0.0067,\n",
       "        0.0078,  0.0089,  0.01  ]), 'alpha': array([  5.00000e-05,   2.67895e-03,   5.30789e-03,   7.93684e-03,\n",
       "         1.0565...,   3.68553e-02,   3.94842e-02,\n",
       "         4.21132e-02,   4.47421e-02,   4.73711e-02,   5.00000e-02])},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score=True, scoring=None, verbose=1)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set the range of hyperparameters\n",
    "param_distributions = {'hidden_layer_sizes': [(50,25),(25,25),(25,10),(50,),(25,),(10,)],\n",
    "                       'alpha': np.linspace(0.00005,0.05,20),\n",
    "                       'tol': np.linspace(1e-4,1e-2,10)}\n",
    "# initialize the random search\n",
    "random_search = RandomizedSearchCV(estimator=MLPClassifier(solver='lbfgs'),\n",
    "                                   param_distributions=param_distributions,\n",
    "                                   n_iter=30,\n",
    "                                   cv=10,\n",
    "                                   verbose=1)\n",
    "# start searching\n",
    "random_search.fit(x_train.astype(np.float), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_layer_sizes': (25,), 'tol': 0.0078000000000000005, 'alpha': 0.010565789473684211}\n",
      "Best accuracy:  0.826038159371\n",
      "MLPClassifier(activation='relu', alpha=0.010565789473684211,\n",
      "       batch_size='auto', beta_1=0.9, beta_2=0.999, early_stopping=False,\n",
      "       epsilon=1e-08, hidden_layer_sizes=(25,), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
      "       shuffle=True, solver='lbfgs', tol=0.0078000000000000005,\n",
      "       validation_fraction=0.1, verbose=False, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print(random_search.best_params_)\n",
    "print('Best accuracy: ', random_search.best_score_)\n",
    "print(random_search.best_estimator_)\n",
    "mlp = random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the model parameters\n",
    "mlp = MLPClassifier(activation='relu', alpha=0.010565789473684211,\n",
    "       batch_size='auto', beta_1=0.9, beta_2=0.999, early_stopping=False,\n",
    "       epsilon=1e-08, hidden_layer_sizes=(25,), learning_rate='constant',\n",
    "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
    "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
    "       shuffle=True, solver='lbfgs', tol=0.0078000000000000005,\n",
    "       validation_fraction=0.1, verbose=False, warm_start=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed: 11.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, error_score='raise',\n",
       "          estimator=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "              warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=30, n_jobs=1,\n",
       "          param_distributions={'max_depth': array([  2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.]), 'learning_rate': array([ 0.001  ,  0.01109, ...,  0.98991,  1.     ]), 'subsample': array([ 0.1    ,  0.14737,  0.19474,  0.24211,  0.28947,  0.33684,\n",
       "        0.38421,  0.43158,  0.47895,  0.52632,  0.57...    19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
       "       36, 37, 38, 39, 40])},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score=True, scoring=None, verbose=1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set the range of hyperparameters\n",
    "param_distributions = {'learning_rate': np.linspace(1e-3,1,100),\n",
    "                       'n_estimators': np.around(np.linspace(50,1000,100)).astype(np.int),\n",
    "                       'max_depth': np.linspace(2,10,9),\n",
    "                       'min_samples_split': np.linspace(2,40,39).astype(np.int),\n",
    "                       'max_features': np.linspace(1,7,7).astype(np.int),\n",
    "                       'subsample': np.linspace(0.1,1,20)}\n",
    "# initialize the random search\n",
    "random_search = RandomizedSearchCV(estimator=GradientBoostingClassifier(),\n",
    "                                   param_distributions=param_distributions,\n",
    "                                   n_iter=30,\n",
    "                                   cv=10,\n",
    "                                   verbose=1)\n",
    "# start searching\n",
    "random_search.fit(x_train.astype(np.float), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 6.0, 'learning_rate': 0.011090909090909092, 'subsample': 0.14736842105263159, 'n_estimators': 587, 'max_features': 7, 'min_samples_split': 25}\n",
      "Best accuracy:  0.832772166105\n",
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.011090909090909092, loss='deviance',\n",
      "              max_depth=6.0, max_features=7, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=25,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=587,\n",
      "              presort='auto', random_state=None,\n",
      "              subsample=0.14736842105263159, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "# see the best\n",
    "print(random_search.best_params_)\n",
    "print('Best accuracy: ', random_search.best_score_)\n",
    "print(random_search.best_estimator_)\n",
    "gbm = random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the model parameters\n",
    "gbm = GradientBoostingClassifier(learning_rate=0.011090909090909092,\n",
    "                                 max_depth=6,\n",
    "                                 max_features=7,\n",
    "                                 min_samples_split=25,\n",
    "                                 n_estimators=587,\n",
    "                                 subsample=0.14736842105263159)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Have the collections of models\n",
    "mlp = MLPClassifier(alpha=0.010565789473684211,\n",
    "                    hidden_layer_sizes=(25,),\n",
    "                    solver='lbfgs',\n",
    "                    tol=0.0078000000000000005)\n",
    "svm = SVC(C=0.90526315789473688,\n",
    "          gamma=0.59999999999999998,\n",
    "          probability=True,\n",
    "          tol=0.0067000000000000002)\n",
    "rf = RandomForestClassifier(max_features=5,\n",
    "                            min_impurity_decrease=0.0,\n",
    "                            min_samples_leaf=9, \n",
    "                            min_samples_split=2,\n",
    "                            n_estimators=265)\n",
    "knn = KNeighborsClassifier(leaf_size=50,\n",
    "                           n_neighbors=9,\n",
    "                           p=2)\n",
    "gbm = GradientBoostingClassifier(learning_rate=0.011090909090909092,\n",
    "                                 max_depth=6,\n",
    "                                 max_features=7,\n",
    "                                 min_samples_split=25,\n",
    "                                 n_estimators=587,\n",
    "                                 subsample=0.14736842105263159)\n",
    "log = LogisticRegression(C=0.31111111111111112, class_weight=None, dual=False,\n",
    "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
    "          multi_class='ovr', n_jobs=1, penalty='l1', random_state=None,\n",
    "          solver='liblinear', tol=1.0000000000000001e-05, verbose=0,\n",
    "          warm_start=False)\n",
    "nb = GaussianNB()\n",
    "\n",
    "# fit and produce predictions\n",
    "classifiers = [mlp, svm, rf, knn, gbm, nb, log]\n",
    "pred = []\n",
    "for clf in classifiers:\n",
    "    clf.fit(x_train.astype(np.float), y_train)\n",
    "    pred.append(clf.predict(x_train.astype(np.float)).astype(np.float))\n",
    "pred = np.array(pred).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the correlation of prediction results using Chi-square test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 7)\n",
      "[[ 1.          0.81258949  0.82598103  0.78985123  0.81125225  0.72023638\n",
      "   0.77886047]\n",
      " [ 0.81258949  1.          0.8646752   0.86801047  0.85391864  0.68270761\n",
      "   0.70145759]\n",
      " [ 0.82598103  0.8646752   1.          0.86046002  0.90641598  0.68855527\n",
      "   0.73401199]\n",
      " [ 0.78985123  0.86801047  0.86046002  1.          0.85207741  0.68607654\n",
      "   0.66199908]\n",
      " [ 0.81125225  0.85391864  0.90641598  0.85207741  1.          0.72705634\n",
      "   0.7263311 ]\n",
      " [ 0.72023638  0.68270761  0.68855527  0.68607654  0.72705634  1.\n",
      "   0.7804306 ]\n",
      " [ 0.77886047  0.70145759  0.73401199  0.66199908  0.7263311   0.7804306\n",
      "   1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(pred.shape)\n",
    "print(np.corrcoef(pred, rowvar=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we combine some of the best models above to work together and vote. Specificially, we would use trained **Random Forest**, **SVM**, **k-nearest neighbors** and **Multilayer Perceptrons** to form an ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('RF', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features=5, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=9, min_samples_split=2,\n",
       "            min_weight_...bfgs', tol=0.0078000000000000005,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False))],\n",
       "         flatten_transform=None, n_jobs=1, voting='soft', weights=None)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ens = VotingClassifier(estimators=[('RF',rf),('SVM',svm),('kNN',knn),('mlp',mlp)], \n",
    "                       voting='soft')\n",
    "ens.fit(x_train.astype(np.float), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0' '1' '0' '0' '1' '0' '1' '0' '1' '0']\n"
     ]
    }
   ],
   "source": [
    "print(ens.predict(x_test.astype(np.float)[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the prediction results as features to train another classifier again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:  8.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, error_score='raise',\n",
       "          estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=30, n_jobs=1,\n",
       "          param_distributions={'min_samples_split': array([ 2,  3,  4,  5,  6,  7,  8,  9, 10]), 'n_estimators': array([ 10,  11, ..., 499, 500]), 'max_features': array([1, 2, 3, 4, 5]), 'min_impurity_decrease': array([ 0.     ,  0.00526,  0.01053,  0.01579,  0.02105,  0.02632,\n",
       "        0.03158,  0.03684,  0.04211,  0.04737,  0.05263,  0.05789,\n",
       "        0.06316,  0.06842,  0.07368,  0.07895,  0.08421,  0.08947,\n",
       "        0.09474,  0.1    ]), 'min_samples_leaf': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score=True, scoring=None, verbose=1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use random forest as second level classifier, again random search\n",
    "# set the range of hyperparameters\n",
    "param_distributions = {'n_estimators': np.array(np.arange(10,501), dtype=np.int),\n",
    "                       'max_features': np.array(np.linspace(1,5,5), dtype=np.int),\n",
    "                       'min_samples_split': np.array(np.linspace(2,10,9), dtype=np.int),\n",
    "                       'min_samples_leaf': np.array(np.linspace(1,10,10), dtype=np.int),\n",
    "                       'min_impurity_decrease': np.linspace(0,0.1,20)}\n",
    "# initialize the random search\n",
    "random_search = RandomizedSearchCV(estimator=RandomForestClassifier(),\n",
    "                                   param_distributions=param_distributions,\n",
    "                                   n_iter=30,\n",
    "                                   cv=10,\n",
    "                                   verbose=1)\n",
    "# start searching\n",
    "random_search.fit(pred, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min_samples_leaf': 8, 'n_estimators': 304, 'min_samples_split': 10, 'min_impurity_decrease': 0.094736842105263161, 'max_features': 3}\n",
      "Best accuracy:  0.881032547699\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=3, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.094736842105263161,\n",
      "            min_impurity_split=None, min_samples_leaf=8,\n",
      "            min_samples_split=10, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=304, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print(random_search.best_params_)\n",
    "print('Best accuracy: ', random_search.best_score_)\n",
    "print(random_search.best_estimator_)\n",
    "stack_2 = random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network using Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have more flexibility, we use Keras instead of Multilayer Perceptron in scikit_learn.\n",
    "\n",
    "In concern with model selection, since the data only has 7 features and is not time series, both CNN and RNN are not quite suitable, therefore only MLP(Multilayer Perceptron) is appropriate.\n",
    "\n",
    "First, define the mlp model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mlp_model(hidden_dims, dropout=0.5, lr=0.001):\n",
    "    # build the model\n",
    "    mlp = Sequential()\n",
    "    mlp.add(Flatten(input_shape=(7,1)))\n",
    "    for hidden_dim in hidden_dims:\n",
    "        mlp.add(Dense(units=hidden_dim, activation='relu'))\n",
    "        if dropout:\n",
    "            mlp.add(Dropout(rate=dropout))\n",
    "    mlp.add(Dense(units=2, activation='softmax'))\n",
    "    \n",
    "    # set the optimizer and loss\n",
    "    rmsprop = RMSprop(lr=lr, decay=0.99)\n",
    "    mlp.compile(loss='sparse_categorical_crossentropy',\n",
    "                       optimizer=rmsprop, metrics=['accuracy'])\n",
    "    return mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then set the early stopping and train the model. 91 observations are used for monitoring, and 800 observations are used to train. Here we just try usual hyperparameters to train and see the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 91 samples\n",
      "Epoch 1/100\n",
      "800/800 [==============================] - 0s - loss: 0.6687 - acc: 0.6238 - val_loss: 0.6526 - val_acc: 0.6264\n",
      "Epoch 2/100\n",
      "800/800 [==============================] - 0s - loss: 0.6686 - acc: 0.6238 - val_loss: 0.6505 - val_acc: 0.6264\n",
      "Epoch 3/100\n",
      "800/800 [==============================] - 0s - loss: 0.6725 - acc: 0.6212 - val_loss: 0.6493 - val_acc: 0.6264\n",
      "Epoch 4/100\n",
      "800/800 [==============================] - 0s - loss: 0.6782 - acc: 0.5975 - val_loss: 0.6483 - val_acc: 0.6264\n",
      "Epoch 5/100\n",
      "800/800 [==============================] - 0s - loss: 0.6743 - acc: 0.6075 - val_loss: 0.6477 - val_acc: 0.6264\n",
      "Epoch 6/100\n",
      "800/800 [==============================] - 0s - loss: 0.6659 - acc: 0.6288 - val_loss: 0.6471 - val_acc: 0.6264\n",
      "Epoch 7/100\n",
      "800/800 [==============================] - 0s - loss: 0.6724 - acc: 0.6075 - val_loss: 0.6466 - val_acc: 0.6264\n",
      "Epoch 8/100\n",
      "800/800 [==============================] - 0s - loss: 0.6787 - acc: 0.6112 - val_loss: 0.6461 - val_acc: 0.6264\n",
      "Epoch 9/100\n",
      "800/800 [==============================] - 0s - loss: 0.6677 - acc: 0.6163 - val_loss: 0.6457 - val_acc: 0.6264\n",
      "Epoch 10/100\n",
      "800/800 [==============================] - 0s - loss: 0.6602 - acc: 0.6313 - val_loss: 0.6454 - val_acc: 0.6264\n",
      "Epoch 11/100\n",
      "800/800 [==============================] - 0s - loss: 0.6835 - acc: 0.6050 - val_loss: 0.6451 - val_acc: 0.6264\n",
      "Epoch 12/100\n",
      "800/800 [==============================] - 0s - loss: 0.6792 - acc: 0.6025 - val_loss: 0.6448 - val_acc: 0.6264\n",
      "Epoch 13/100\n",
      "800/800 [==============================] - 0s - loss: 0.6735 - acc: 0.6262 - val_loss: 0.6446 - val_acc: 0.6264\n",
      "Epoch 14/100\n",
      "800/800 [==============================] - 0s - loss: 0.6667 - acc: 0.6200 - val_loss: 0.6443 - val_acc: 0.6264\n",
      "Epoch 15/100\n",
      "800/800 [==============================] - 0s - loss: 0.6721 - acc: 0.6200 - val_loss: 0.6441 - val_acc: 0.6264\n",
      "Epoch 16/100\n",
      "800/800 [==============================] - 0s - loss: 0.6761 - acc: 0.6375 - val_loss: 0.6439 - val_acc: 0.6264\n",
      "Epoch 17/100\n",
      "800/800 [==============================] - 0s - loss: 0.6746 - acc: 0.6337 - val_loss: 0.6437 - val_acc: 0.6264\n",
      "Epoch 18/100\n",
      "800/800 [==============================] - 0s - loss: 0.6592 - acc: 0.6238 - val_loss: 0.6436 - val_acc: 0.6264\n",
      "Epoch 19/100\n",
      "800/800 [==============================] - 0s - loss: 0.6609 - acc: 0.6175 - val_loss: 0.6434 - val_acc: 0.6264\n",
      "Epoch 20/100\n",
      "800/800 [==============================] - 0s - loss: 0.6591 - acc: 0.6337 - val_loss: 0.6432 - val_acc: 0.6264\n",
      "Epoch 21/100\n",
      "800/800 [==============================] - 0s - loss: 0.6566 - acc: 0.6375 - val_loss: 0.6431 - val_acc: 0.6264\n",
      "Epoch 22/100\n",
      "800/800 [==============================] - 0s - loss: 0.6709 - acc: 0.6125 - val_loss: 0.6429 - val_acc: 0.6264\n",
      "Epoch 23/100\n",
      "800/800 [==============================] - 0s - loss: 0.6626 - acc: 0.6225 - val_loss: 0.6428 - val_acc: 0.6264\n",
      "Epoch 24/100\n",
      "800/800 [==============================] - 0s - loss: 0.6627 - acc: 0.6187 - val_loss: 0.6426 - val_acc: 0.6264\n",
      "Epoch 25/100\n",
      "800/800 [==============================] - 0s - loss: 0.6700 - acc: 0.5988 - val_loss: 0.6425 - val_acc: 0.6264\n",
      "Epoch 26/100\n",
      "800/800 [==============================] - 0s - loss: 0.6746 - acc: 0.6163 - val_loss: 0.6424 - val_acc: 0.6264\n",
      "Epoch 27/100\n",
      "800/800 [==============================] - 0s - loss: 0.6658 - acc: 0.6112 - val_loss: 0.6423 - val_acc: 0.6264\n",
      "Epoch 28/100\n",
      "800/800 [==============================] - 0s - loss: 0.6622 - acc: 0.6225 - val_loss: 0.6422 - val_acc: 0.6264\n",
      "Epoch 29/100\n",
      "800/800 [==============================] - 0s - loss: 0.6640 - acc: 0.6200 - val_loss: 0.6420 - val_acc: 0.6264\n",
      "Epoch 30/100\n",
      "800/800 [==============================] - 0s - loss: 0.6669 - acc: 0.6138 - val_loss: 0.6419 - val_acc: 0.6264\n",
      "Epoch 31/100\n",
      "800/800 [==============================] - 0s - loss: 0.6602 - acc: 0.6187 - val_loss: 0.6418 - val_acc: 0.6264\n",
      "Epoch 32/100\n",
      "800/800 [==============================] - 0s - loss: 0.6516 - acc: 0.6238 - val_loss: 0.6417 - val_acc: 0.6264\n",
      "Epoch 33/100\n",
      "800/800 [==============================] - 0s - loss: 0.6717 - acc: 0.6225 - val_loss: 0.6416 - val_acc: 0.6264\n",
      "Epoch 34/100\n",
      "800/800 [==============================] - 0s - loss: 0.6632 - acc: 0.6250 - val_loss: 0.6415 - val_acc: 0.6264\n",
      "Epoch 35/100\n",
      "800/800 [==============================] - 0s - loss: 0.6591 - acc: 0.6212 - val_loss: 0.6415 - val_acc: 0.6264\n",
      "Epoch 36/100\n",
      "800/800 [==============================] - 0s - loss: 0.6634 - acc: 0.6100 - val_loss: 0.6414 - val_acc: 0.6264\n",
      "Epoch 37/100\n",
      "800/800 [==============================] - 0s - loss: 0.6731 - acc: 0.6225 - val_loss: 0.6413 - val_acc: 0.6264\n",
      "Epoch 38/100\n",
      "800/800 [==============================] - 0s - loss: 0.6692 - acc: 0.6187 - val_loss: 0.6412 - val_acc: 0.6264\n",
      "Epoch 39/100\n",
      "800/800 [==============================] - 0s - loss: 0.6658 - acc: 0.6125 - val_loss: 0.6411 - val_acc: 0.6264\n",
      "Epoch 40/100\n",
      "800/800 [==============================] - 0s - loss: 0.6723 - acc: 0.6038 - val_loss: 0.6410 - val_acc: 0.6264\n",
      "Epoch 41/100\n",
      "800/800 [==============================] - 0s - loss: 0.6720 - acc: 0.6163 - val_loss: 0.6410 - val_acc: 0.6264\n",
      "Epoch 42/100\n",
      "800/800 [==============================] - 0s - loss: 0.6594 - acc: 0.6225 - val_loss: 0.6409 - val_acc: 0.6264\n",
      "Epoch 43/100\n",
      "800/800 [==============================] - 0s - loss: 0.6641 - acc: 0.6262 - val_loss: 0.6408 - val_acc: 0.6264\n",
      "Epoch 44/100\n",
      "800/800 [==============================] - 0s - loss: 0.6553 - acc: 0.6212 - val_loss: 0.6407 - val_acc: 0.6264\n",
      "Epoch 45/100\n",
      "800/800 [==============================] - 0s - loss: 0.6663 - acc: 0.6238 - val_loss: 0.6407 - val_acc: 0.6264\n",
      "Epoch 46/100\n",
      "800/800 [==============================] - 0s - loss: 0.6535 - acc: 0.6412 - val_loss: 0.6406 - val_acc: 0.6264\n",
      "Epoch 47/100\n",
      "800/800 [==============================] - 0s - loss: 0.6624 - acc: 0.6300 - val_loss: 0.6405 - val_acc: 0.6264\n",
      "Epoch 48/100\n",
      "800/800 [==============================] - 0s - loss: 0.6706 - acc: 0.6088 - val_loss: 0.6405 - val_acc: 0.6264\n",
      "Epoch 49/100\n",
      "800/800 [==============================] - 0s - loss: 0.6731 - acc: 0.6175 - val_loss: 0.6404 - val_acc: 0.6264\n",
      "Epoch 50/100\n",
      "800/800 [==============================] - 0s - loss: 0.6450 - acc: 0.6350 - val_loss: 0.6403 - val_acc: 0.6264\n",
      "Epoch 51/100\n",
      "800/800 [==============================] - 0s - loss: 0.6616 - acc: 0.6250 - val_loss: 0.6403 - val_acc: 0.6264\n",
      "Epoch 52/100\n",
      "800/800 [==============================] - 0s - loss: 0.6669 - acc: 0.6212 - val_loss: 0.6402 - val_acc: 0.6264\n",
      "Epoch 53/100\n",
      "800/800 [==============================] - 0s - loss: 0.6659 - acc: 0.6313 - val_loss: 0.6402 - val_acc: 0.6264\n",
      "Epoch 54/100\n",
      "800/800 [==============================] - 0s - loss: 0.6687 - acc: 0.6225 - val_loss: 0.6401 - val_acc: 0.6264\n",
      "Epoch 55/100\n",
      "800/800 [==============================] - 0s - loss: 0.6676 - acc: 0.6187 - val_loss: 0.6400 - val_acc: 0.6264\n",
      "Epoch 56/100\n",
      "800/800 [==============================] - 0s - loss: 0.6640 - acc: 0.6313 - val_loss: 0.6400 - val_acc: 0.6264\n",
      "Epoch 57/100\n",
      "800/800 [==============================] - 0s - loss: 0.6640 - acc: 0.6200 - val_loss: 0.6399 - val_acc: 0.6264\n",
      "Epoch 58/100\n",
      "800/800 [==============================] - 0s - loss: 0.6571 - acc: 0.6125 - val_loss: 0.6399 - val_acc: 0.6264\n",
      "Epoch 59/100\n",
      "800/800 [==============================] - 0s - loss: 0.6574 - acc: 0.6300 - val_loss: 0.6398 - val_acc: 0.6264\n",
      "Epoch 60/100\n",
      "800/800 [==============================] - 0s - loss: 0.6559 - acc: 0.6200 - val_loss: 0.6398 - val_acc: 0.6264\n",
      "Epoch 61/100\n",
      "800/800 [==============================] - 0s - loss: 0.6672 - acc: 0.6088 - val_loss: 0.6397 - val_acc: 0.6264\n",
      "Epoch 62/100\n",
      "800/800 [==============================] - 0s - loss: 0.6571 - acc: 0.6175 - val_loss: 0.6397 - val_acc: 0.6264\n",
      "Epoch 63/100\n",
      "800/800 [==============================] - 0s - loss: 0.6637 - acc: 0.6062 - val_loss: 0.6396 - val_acc: 0.6264\n",
      "Epoch 64/100\n",
      "800/800 [==============================] - 0s - loss: 0.6576 - acc: 0.6187 - val_loss: 0.6396 - val_acc: 0.6264\n",
      "Epoch 65/100\n",
      "800/800 [==============================] - 0s - loss: 0.6620 - acc: 0.6150 - val_loss: 0.6395 - val_acc: 0.6264\n",
      "Epoch 66/100\n",
      "800/800 [==============================] - 0s - loss: 0.6755 - acc: 0.5988 - val_loss: 0.6395 - val_acc: 0.6264\n",
      "Epoch 67/100\n",
      "800/800 [==============================] - 0s - loss: 0.6611 - acc: 0.6275 - val_loss: 0.6394 - val_acc: 0.6264\n",
      "Epoch 68/100\n",
      "800/800 [==============================] - 0s - loss: 0.6565 - acc: 0.6362 - val_loss: 0.6394 - val_acc: 0.6264\n",
      "Epoch 69/100\n",
      "800/800 [==============================] - 0s - loss: 0.6664 - acc: 0.6238 - val_loss: 0.6393 - val_acc: 0.6264\n",
      "Epoch 70/100\n",
      "800/800 [==============================] - 0s - loss: 0.6571 - acc: 0.6313 - val_loss: 0.6393 - val_acc: 0.6264\n",
      "Epoch 71/100\n",
      "800/800 [==============================] - 0s - loss: 0.6592 - acc: 0.6125 - val_loss: 0.6392 - val_acc: 0.6264\n",
      "Epoch 72/100\n",
      "800/800 [==============================] - 0s - loss: 0.6624 - acc: 0.6225 - val_loss: 0.6392 - val_acc: 0.6264\n",
      "Epoch 73/100\n",
      "800/800 [==============================] - 0s - loss: 0.6474 - acc: 0.6438 - val_loss: 0.6392 - val_acc: 0.6264\n",
      "Epoch 74/100\n",
      "800/800 [==============================] - 0s - loss: 0.6694 - acc: 0.6187 - val_loss: 0.6391 - val_acc: 0.6264\n",
      "Epoch 75/100\n",
      "800/800 [==============================] - 0s - loss: 0.6564 - acc: 0.6250 - val_loss: 0.6391 - val_acc: 0.6264\n",
      "Epoch 76/100\n",
      "800/800 [==============================] - 0s - loss: 0.6685 - acc: 0.6212 - val_loss: 0.6390 - val_acc: 0.6264\n",
      "Epoch 77/100\n",
      "800/800 [==============================] - 0s - loss: 0.6691 - acc: 0.6275 - val_loss: 0.6390 - val_acc: 0.6264\n",
      "Epoch 78/100\n",
      "800/800 [==============================] - 0s - loss: 0.6595 - acc: 0.6100 - val_loss: 0.6390 - val_acc: 0.6264\n",
      "Epoch 79/100\n",
      "800/800 [==============================] - 0s - loss: 0.6508 - acc: 0.6225 - val_loss: 0.6389 - val_acc: 0.6264\n",
      "Epoch 80/100\n",
      "800/800 [==============================] - 0s - loss: 0.6718 - acc: 0.6088 - val_loss: 0.6389 - val_acc: 0.6264\n",
      "Epoch 81/100\n",
      "800/800 [==============================] - 0s - loss: 0.6549 - acc: 0.6200 - val_loss: 0.6388 - val_acc: 0.6264\n",
      "Epoch 82/100\n",
      "800/800 [==============================] - 0s - loss: 0.6538 - acc: 0.6337 - val_loss: 0.6388 - val_acc: 0.6264\n",
      "Epoch 83/100\n",
      "800/800 [==============================] - 0s - loss: 0.6643 - acc: 0.6212 - val_loss: 0.6388 - val_acc: 0.6264\n",
      "Epoch 84/100\n",
      "800/800 [==============================] - 0s - loss: 0.6722 - acc: 0.6238 - val_loss: 0.6387 - val_acc: 0.6264\n",
      "Epoch 85/100\n",
      "800/800 [==============================] - 0s - loss: 0.6646 - acc: 0.6200 - val_loss: 0.6387 - val_acc: 0.6264\n",
      "Epoch 86/100\n",
      "800/800 [==============================] - 0s - loss: 0.6576 - acc: 0.6275 - val_loss: 0.6386 - val_acc: 0.6264\n",
      "Epoch 87/100\n",
      "800/800 [==============================] - 0s - loss: 0.6624 - acc: 0.6212 - val_loss: 0.6386 - val_acc: 0.6264\n",
      "Epoch 88/100\n",
      "800/800 [==============================] - 0s - loss: 0.6710 - acc: 0.6150 - val_loss: 0.6386 - val_acc: 0.6264\n",
      "Epoch 89/100\n",
      "800/800 [==============================] - 0s - loss: 0.6675 - acc: 0.6200 - val_loss: 0.6385 - val_acc: 0.6264\n",
      "Epoch 90/100\n",
      "800/800 [==============================] - 0s - loss: 0.6572 - acc: 0.6313 - val_loss: 0.6385 - val_acc: 0.6264\n",
      "Epoch 91/100\n",
      "800/800 [==============================] - 0s - loss: 0.6511 - acc: 0.6337 - val_loss: 0.6385 - val_acc: 0.6264\n",
      "Epoch 92/100\n",
      "800/800 [==============================] - 0s - loss: 0.6536 - acc: 0.6238 - val_loss: 0.6384 - val_acc: 0.6264\n",
      "Epoch 93/100\n",
      "800/800 [==============================] - 0s - loss: 0.6650 - acc: 0.6125 - val_loss: 0.6384 - val_acc: 0.6264\n",
      "Epoch 94/100\n",
      "800/800 [==============================] - 0s - loss: 0.6519 - acc: 0.6412 - val_loss: 0.6384 - val_acc: 0.6264\n",
      "Epoch 95/100\n",
      "800/800 [==============================] - 0s - loss: 0.6495 - acc: 0.6200 - val_loss: 0.6383 - val_acc: 0.6264\n",
      "Epoch 96/100\n",
      "800/800 [==============================] - 0s - loss: 0.6652 - acc: 0.6362 - val_loss: 0.6383 - val_acc: 0.6264\n",
      "Epoch 97/100\n",
      "800/800 [==============================] - 0s - loss: 0.6502 - acc: 0.6088 - val_loss: 0.6383 - val_acc: 0.6264\n",
      "Epoch 98/100\n",
      "800/800 [==============================] - 0s - loss: 0.6602 - acc: 0.6400 - val_loss: 0.6382 - val_acc: 0.6264\n",
      "Epoch 99/100\n",
      "800/800 [==============================] - 0s - loss: 0.6646 - acc: 0.6125 - val_loss: 0.6382 - val_acc: 0.6264\n",
      "Epoch 100/100\n",
      "800/800 [==============================] - 0s - loss: 0.6598 - acc: 0.6262 - val_loss: 0.6382 - val_acc: 0.6264\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbd7346d940>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earlystop = EarlyStopping(min_delta=0.0001, patience=5)\n",
    "\n",
    "# use test data to monitor early stopping\n",
    "model = mlp_model(hidden_dims=[50,25],\n",
    "                  dropout=0.2,\n",
    "                  lr=0.001)\n",
    "model.fit(x_train[:800].reshape(-1,7,1), y_train[:800].reshape(-1,1),\n",
    "               batch_size=32,\n",
    "               epochs=100,\n",
    "               validation_data=(x_train[800:].reshape(-1,7,1), y_train[800:].reshape(-1,1)),\n",
    "               callbacks=[earlystop],\n",
    "               initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The result above is actually not satisfied, so we use random search to find the best set of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_parameters={'lr': 0.001, 'dropout': 0.5, 'hidden_dims': []}\n",
    "lowest_err = 1000\n",
    "lr_range = (0.1,0.0001); dropout_range = (0.3,0.8); dense_range = (512,2048)\n",
    "while True:\n",
    "    lr = np.random.uniform(lr_range[0], lr_range[1])\n",
    "    dropout = np.random.uniform(dropout_range[0], dropout_range[1])\n",
    "    dense_dim = int(np.random.uniform(dense_range[0], dense_range[1]))\n",
    "    drop_conv, avgpool = np.random.binomial(1,0.5,2)\n",
    "    ktf.clear_session()\n",
    "    test_err, test_acc = simpleCNN_model(lr, dropout, dense_dim, drop_conv, avgpool)\n",
    "    if test_err < lowest_err:\n",
    "        print('new lowest: ', round(test_err,2), round(test_acc,2), \n",
    "              (round(lr,4), round(dropout,2), dense_dim, bool(drop_conv), bool(avgpool)))\n",
    "        lowest_err = test_err\n",
    "        best_parameters['lr'] = lr\n",
    "        best_parameters['dropout'] = dropout\n",
    "        best_parameters['dense_dim'] = dense_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is used to generate submission file for Kaggle competition using trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create submission file\n",
    "create_submission(ens, x_test.astype(np.float), '../submission/submission_ens2_new_tf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(418, 5)\n"
     ]
    }
   ],
   "source": [
    "# for stacking\n",
    "classifiers = [mlp, svm, rf, knn, gbm]\n",
    "pred = []\n",
    "for clf in classifiers:\n",
    "    pred.append(clf.predict(x_test.astype(np.float)).astype(np.float))\n",
    "    \n",
    "pred = np.array(pred).T\n",
    "print(pred.shape)\n",
    "create_submission(stack_2, pred.astype(np.float), '../submission/submission_stack2_new_tf.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
